"""
æ ‘è“æ´¾å®¢æˆ·ç«¯ä¸»ç¨‹åº - DOAå¢å¼ºç‰ˆ
æ•´åˆ ODAS å£°æºå®šä½åŠŸèƒ½

åŠŸèƒ½:
- è¯­éŸ³äº¤äº’ (ASR + å¯¹è¯ + TTS)
- å®æ—¶å£°æºå®šä½ (DOA)
- å¤šå£°æºè·Ÿè¸ª
- DOA ä¿¡æ¯ä¸åç«¯é›†æˆ
"""

import requests
import logging
import yaml
import argparse
import subprocess
import os
import sys
import time
import threading
from pathlib import Path
from typing import Optional, List, Dict

# æ·»åŠ çˆ¶ç›®å½•ä»¥å¯¼å…¥ acoustic_frontend
sys.path.insert(0, str(Path(__file__).parent))

from audio_capture import AudioCapture
from audio_player import AudioPlayer
from wakeword_detector import WakeWordDetector

# å¯¼å…¥å£°å­¦å‰ç«¯æ¨¡å—
from acoustic_frontend.odas_client import ODASClient, TrackedSource
from acoustic_frontend.beamformer import Beamformer
import numpy as np
import wave
import io

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('client_doa.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class MultiChannelCapture:
    """
    å¤šé€šé“éŸ³é¢‘é‡‡é›†
    
    ä½¿ç”¨ arecord é‡‡é›† 8 é€šé“åŸå§‹éŸ³é¢‘ï¼Œç”¨äºæ³¢æŸæˆå½¢
    """
    
    def __init__(self, card_id: int = 3, sample_rate: int = 16000, num_channels: int = 8):
        self.card_id = card_id
        self.sample_rate = sample_rate
        self.num_channels = num_channels
        self.bytes_per_sample = 4  # S32_LE
        
        # WebRTC VAD (æ›´å‡†ç¡®çš„è¯­éŸ³ç«¯ç‚¹æ£€æµ‹)
        self._vad = None
        try:
            import webrtcvad
            self._vad = webrtcvad.Vad(2)  # æ¨¡å¼ 2: ä¸­ç­‰çµæ•åº¦
            logger.info("MultiChannelCapture: WebRTC VAD enabled")
        except ImportError:
            logger.warning("webrtcvad not installed, using energy-based VAD")
        
    def record(self, duration: float = 5.0) -> Optional[np.ndarray]:
        """
        å½•åˆ¶å¤šé€šé“éŸ³é¢‘
        
        Args:
            duration: å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
            
        Returns:
            éŸ³é¢‘æ•°æ® shape=(samples, 8)ï¼Œæˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
        """
        try:
            cmd = [
                'arecord',
                '-D', f'hw:{self.card_id},0',
                '-f', 'S32_LE',
                '-r', str(self.sample_rate),
                '-c', str(self.num_channels),
                '-d', str(int(duration)),
                '-t', 'raw',
                '-q',
                '-'
            ]
            
            result = subprocess.run(cmd, capture_output=True, timeout=duration + 5)
            
            if result.returncode != 0:
                logger.error(f"arecord failed: {result.stderr.decode()}")
                return None
            
            # è§£æåŸå§‹æ•°æ®
            raw_data = result.stdout
            audio = np.frombuffer(raw_data, dtype=np.int32)
            
            # é‡å¡‘ä¸º (samples, channels)
            num_samples = len(audio) // self.num_channels
            audio = audio[:num_samples * self.num_channels].reshape(num_samples, self.num_channels)
            
            # è½¬æ¢ä¸º float32 å¹¶å½’ä¸€åŒ–
            audio = audio.astype(np.float32) / (2**31)
            
            return audio
            
        except subprocess.TimeoutExpired:
            logger.error("arecord timeout")
            return None
        except Exception as e:
            logger.error(f"MultiChannelCapture error: {e}")
            return None
    
    def record_with_vad(
        self, 
        max_duration: float = 30.0,
        silence_duration: float = 1.0,
        energy_threshold: float = 0.005,
        frame_duration_ms: int = 30
    ) -> Optional[np.ndarray]:
        """
        å¸¦ VAD çš„å¤šé€šé“å½•éŸ³
        
        ä½¿ç”¨ WebRTC VAD è¿›è¡Œè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ŒæŒç»­å½•éŸ³ç›´åˆ°æ£€æµ‹åˆ°è¶³å¤Ÿé•¿çš„é™éŸ³
        
        Args:
            max_duration: æœ€å¤§å½•éŸ³æ—¶é•¿
            silence_duration: é™éŸ³åˆ¤å®šæ—¶é•¿ï¼ˆç§’ï¼‰
            energy_threshold: èƒ½é‡é˜ˆå€¼ï¼ˆå¤‡ç”¨ï¼‰
            frame_duration_ms: VAD å¸§é•¿åº¦ï¼ˆmsï¼Œå¿…é¡»æ˜¯ 10/20/30ï¼‰
            
        Returns:
            éŸ³é¢‘æ•°æ® shape=(samples, 8)
        """
        try:
            cmd = [
                'arecord',
                '-D', f'hw:{self.card_id},0',
                '-f', 'S32_LE',
                '-r', str(self.sample_rate),
                '-c', str(self.num_channels),
                '-t', 'raw',
                '-q',
                '-'
            ]
            
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            chunks = []
            # VAD å¸§: 30ms = 480 samples @ 16kHz
            frame_samples = int(self.sample_rate * frame_duration_ms / 1000)
            frame_bytes = frame_samples * self.num_channels * self.bytes_per_sample
            
            silence_frames = 0
            speech_started = False
            total_samples = 0
            max_samples = int(max_duration * self.sample_rate)
            silence_frames_threshold = int(silence_duration * 1000 / frame_duration_ms)
            
            logger.info("MultiChannelCapture: VAD recording started")
            
            while total_samples < max_samples:
                raw_data = process.stdout.read(frame_bytes)
                if not raw_data or len(raw_data) < frame_bytes:
                    break
                
                # è§£æ S32_LE â†’ int32
                audio_frame = np.frombuffer(raw_data, dtype=np.int32)
                num_samples = len(audio_frame) // self.num_channels
                audio_frame = audio_frame[:num_samples * self.num_channels].reshape(num_samples, self.num_channels)
                
                total_samples += num_samples
                
                # æå–ç¬¬ä¸€ä¸ªéº¦å…‹é£é€šé“åš VADï¼ˆéœ€è¦è½¬ 16-bitï¼‰
                mono_float = audio_frame[:, 0].astype(np.float32) / (2**31)
                mono_int16 = (mono_float * 32767).astype(np.int16)
                
                # åˆ¤æ–­æ˜¯å¦æœ‰è¯­éŸ³
                is_speech = False
                if self._vad is not None:
                    try:
                        is_speech = self._vad.is_speech(mono_int16.tobytes(), self.sample_rate)
                    except Exception:
                        # VAD å¤±è´¥ï¼Œç”¨èƒ½é‡åˆ¤æ–­
                        energy = np.sqrt(np.mean(mono_float ** 2))
                        is_speech = energy > energy_threshold
                else:
                    # æ—  VADï¼Œç”¨èƒ½é‡åˆ¤æ–­
                    energy = np.sqrt(np.mean(mono_float ** 2))
                    is_speech = energy > energy_threshold
                
                if is_speech:
                    speech_started = True
                    silence_frames = 0
                    chunks.append(audio_frame)
                elif speech_started:
                    chunks.append(audio_frame)
                    silence_frames += 1
                    if silence_frames >= silence_frames_threshold:
                        logger.info(f"MultiChannelCapture: Speech ended after {total_samples/self.sample_rate:.1f}s")
                        break
            
            process.terminate()
            try:
                process.wait(timeout=2)
            except subprocess.TimeoutExpired:
                process.kill()
            
            if not chunks:
                logger.warning("MultiChannelCapture: No speech detected")
                return None
            
            # åˆå¹¶
            audio = np.vstack(chunks)
            audio_float = audio.astype(np.float32) / (2**31)
            
            logger.info(f"MultiChannelCapture: Recorded {len(audio_float)/self.sample_rate:.2f}s ({len(chunks)} frames)")
            return audio_float
            
        except Exception as e:
            logger.error(f"MultiChannelCapture VAD error: {e}")
            import traceback
            traceback.print_exc()
            return None


class MicGainManager:
    """éº¦å…‹é£å¢ç›Šç®¡ç†å™¨"""
    
    def __init__(self, card_id: int = 3, adc_gain: int = 8, digital_volume: int = 160):
        self.card_id = card_id
        self.adc_gain = adc_gain
        self.digital_volume = digital_volume
    
    def set_gains(self) -> bool:
        """è®¾ç½®éº¦å…‹é£å¢ç›Š"""
        try:
            for i in range(1, 9):
                subprocess.run(
                    ['amixer', '-c', str(self.card_id), 'cset', f'name=ADC{i} PGA gain', str(self.adc_gain)],
                    capture_output=True, check=False
                )
                subprocess.run(
                    ['amixer', '-c', str(self.card_id), 'cset', f'name=CH{i} digital volume', str(self.digital_volume)],
                    capture_output=True, check=False
                )
            logger.info(f"Mic gains set: ADC={self.adc_gain}, Digital={self.digital_volume}")
            return True
        except Exception as e:
            logger.error(f"Failed to set mic gains: {e}")
            return False
    
    def check_gains(self) -> Dict[str, int]:
        """æ£€æŸ¥å½“å‰å¢ç›Šè®¾ç½®"""
        try:
            result = subprocess.run(
                ['amixer', '-c', str(self.card_id), 'cget', 'name=ADC1 PGA gain'],
                capture_output=True, text=True
            )
            # è§£æ values=X
            for line in result.stdout.split('\n'):
                if 'values=' in line:
                    value = int(line.split('values=')[1].split()[0])
                    return {'adc_gain': value}
            return {}
        except Exception as e:
            logger.error(f"Failed to check gains: {e}")
            return {}


class ODASProcessManager:
    """ODAS è¿›ç¨‹ç®¡ç†å™¨"""
    
    def __init__(self, odas_dir: str = None):
        if odas_dir is None:
            odas_dir = Path(__file__).parent / "odas"
        self.odas_dir = Path(odas_dir)
        self.odas_binary = self.odas_dir / "odas_build" / "build" / "bin" / "odaslive"
        self.config_file = self.odas_dir / "respeaker_6mic.cfg"
        self.process: Optional[subprocess.Popen] = None
        self._log_file: Optional[Path] = Path('/tmp/odas.log')
    
    def start(self) -> bool:
        """å¯åŠ¨ ODAS è¿›ç¨‹"""
        if not self.odas_binary.exists():
            logger.error(f"ODAS binary not found: {self.odas_binary}")
            logger.info("è¯·å…ˆè¿è¡Œ: cd client/odas && ./start_odas.sh install")
            return False
        
        if not self.config_file.exists():
            logger.error(f"ODAS config not found: {self.config_file}")
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²è¿è¡Œ
        if self.process and self.process.poll() is None:
            logger.warning("ODAS already running")
            return True
        
        # æ€æ‰å¯èƒ½å­˜åœ¨çš„æ—§è¿›ç¨‹
        self._kill_existing_odas()
        
        try:
            # åŠ¨æ€æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„å£°å¡ ID
            card_id = self._detect_card_id()
            if card_id is not None:
                runtime_config = self._create_runtime_config(card_id)
                logger.info(f"Detected sound card ID: {card_id}")
            else:
                runtime_config = str(self.config_file)
                logger.warning("Could not detect sound card, using default config")
            
            # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
            log_file = open(self._log_file, 'w')
            
            self.process = subprocess.Popen(
                [str(self.odas_binary), '-c', runtime_config],
                stdout=log_file,
                stderr=subprocess.STDOUT
            )
            
            time.sleep(1.5)  # ç­‰å¾… ODAS å¯åŠ¨
            
            if self.process.poll() is None:
                logger.info(f"ODAS started with PID {self.process.pid}")
                return True
            else:
                logger.error(f"ODAS failed to start, check {self._log_file}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to start ODAS: {e}")
            return False
    
    def _kill_existing_odas(self):
        """æ€æ‰å¯èƒ½å­˜åœ¨çš„æ—§ ODAS è¿›ç¨‹"""
        try:
            subprocess.run(['pkill', '-f', 'odaslive'], capture_output=True)
            time.sleep(0.5)
        except Exception:
            pass
    
    def stop(self):
        """åœæ­¢ ODAS è¿›ç¨‹"""
        if self.process:
            self.process.terminate()
            try:
                self.process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                self.process.kill()
            self.process = None
            logger.info("ODAS stopped")
    
    def is_running(self) -> bool:
        """æ£€æŸ¥ ODAS æ˜¯å¦è¿è¡Œä¸­"""
        return self.process is not None and self.process.poll() is None
    
    def _detect_card_id(self) -> Optional[int]:
        """æ£€æµ‹ ReSpeaker å£°å¡ ID"""
        try:
            result = subprocess.run(['arecord', '-l'], capture_output=True, text=True)
            for line in result.stdout.split('\n'):
                if 'seeed' in line.lower():
                    # æå– card X
                    import re
                    match = re.search(r'card (\d+)', line)
                    if match:
                        return int(match.group(1))
        except Exception:
            pass
        return None
    
    def _create_runtime_config(self, card_id: int) -> str:
        """åˆ›å»ºè¿è¡Œæ—¶é…ç½®æ–‡ä»¶"""
        runtime_config = '/tmp/odas_runtime.cfg'
        try:
            with open(self.config_file, 'r') as f:
                content = f.read()
            
            # æ›¿æ¢ card ID
            import re
            content = re.sub(r'card = \d+;', f'card = {card_id};', content)
            
            with open(runtime_config, 'w') as f:
                f.write(content)
            
            return runtime_config
        except Exception as e:
            logger.warning(f"Failed to create runtime config: {e}")
            return str(self.config_file)


class VoiceAssistantWithDOA:
    """
    è¯­éŸ³åŠ©æ‰‹å®¢æˆ·ç«¯ - DOAå¢å¼ºç‰ˆ
    
    æ•´åˆ ODAS å£°æºå®šä½åŠŸèƒ½
    """
    
    def __init__(self, config_path: str = None):
        # è‡ªåŠ¨æŸ¥æ‰¾é…ç½®æ–‡ä»¶
        if config_path is None:
            possible_paths = [
                Path(__file__).parent.parent / "config" / "config.yaml",
                Path("../config/config.yaml"),
                Path("config/config.yaml"),
            ]
            for p in possible_paths:
                if p.exists():
                    config_path = str(p)
                    break
            else:
                config_path = "../config/config.yaml"
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        
        # æœåŠ¡å™¨åœ°å€
        self.server_url = self.config.get('client', {}).get('server_url', 'http://localhost:5001')
        
        # åˆå§‹åŒ–éŸ³é¢‘æ¨¡å—
        self.capture = AudioCapture(
            sample_rate=self.config.get('audio', {}).get('sample_rate', 16000),
            channels=self.config.get('audio', {}).get('channels', 1)
        )
        self.player = AudioPlayer()
        
        # ä¼šè¯ID
        self.session_id = f"raspberrypi_doa_{int(time.time())}"
        
        # æµå¼ TTS è®¾ç½®
        self.use_streaming_tts = self.config.get('tts', {}).get('streaming', True)
        
        # ===== DOA ç›¸å…³ç»„ä»¶ =====
        # éº¦å…‹é£å¢ç›Šç®¡ç†
        self.mic_gain = MicGainManager(
            card_id=self.config.get('odas', {}).get('card_id', 3),
            adc_gain=self.config.get('odas', {}).get('adc_gain', 8),
            digital_volume=self.config.get('odas', {}).get('digital_volume', 160)
        )
        
        # ODAS è¿›ç¨‹ç®¡ç†
        self.odas_manager = ODASProcessManager()
        
        # ODAS å®¢æˆ·ç«¯ (Python Socket æ¥æ”¶å™¨)
        self.odas_client = ODASClient(
            sst_port=self.config.get('odas', {}).get('sst_port', 9000),
            sss_port=self.config.get('odas', {}).get('ssl_port', 9001)
        )
        
        # DOA çŠ¶æ€
        self._doa_enabled = False
        self._current_sources: List[TrackedSource] = []
        self._doa_lock = threading.Lock()
        
        # ===== æ³¢æŸæˆå½¢ç›¸å…³ç»„ä»¶ =====
        self._beamforming_enabled = self.config.get('beamforming', {}).get('enabled', True)
        
        # å¤šé€šé“éŸ³é¢‘é‡‡é›†
        self.multichannel_capture = MultiChannelCapture(
            card_id=self.config.get('odas', {}).get('card_id', 3),
            sample_rate=self.config.get('audio', {}).get('sample_rate', 16000)
        )
        
        # æ³¢æŸæˆå½¢å™¨
        # ReSpeaker 6-Mic é˜µåˆ—é…ç½®
        # éº¦å…‹é£è§’åº¦: å®˜æ–¹é…ç½®å¯¹åº” 0Â°, 60Â°, 120Â°, 180Â°, 240Â°, 300Â°
        self.beamformer = Beamformer(
            sample_rate=self.config.get('audio', {}).get('sample_rate', 16000),
            mic_angles=[0, 60, 120, 180, 240, 300],
            array_radius=0.0463  # ReSpeaker 6-Mic é˜µåˆ—åŠå¾„
        )
        
        logger.info(f"VoiceAssistantWithDOA initialized")
        logger.info(f"Server: {self.server_url}")
        logger.info(f"Session ID: {self.session_id}")
        logger.info(f"Beamforming: {'enabled' if self._beamforming_enabled else 'disabled'}")
    
    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.warning(f"Failed to load config: {e}. Using defaults.")
            return {}
    
    # ==================== DOA åŠŸèƒ½ ====================
    
    def start_doa(self) -> bool:
        """
        å¯åŠ¨ DOA ç³»ç»Ÿ
        
        å¯åŠ¨é¡ºåº:
        1. è®¾ç½®éº¦å…‹é£å¢ç›Š
        2. å¯åŠ¨ ODAS å®¢æˆ·ç«¯ (Python ç›‘å¬)
        3. å¯åŠ¨ ODAS è¿›ç¨‹ (C++)
        """
        logger.info("Starting DOA system...")
        
        # 1. è®¾ç½®éº¦å…‹é£å¢ç›Š
        self.mic_gain.set_gains()
        
        # 2. å¯åŠ¨ ODAS å®¢æˆ·ç«¯ (å…ˆå¯åŠ¨ï¼Œä½œä¸ºæœåŠ¡å™¨ç­‰å¾… ODAS è¿æ¥)
        self.odas_client.start()
        logger.info("ODAS client started (waiting for ODAS to connect)")
        
        # ç­‰å¾…ç›‘å¬å™¨å°±ç»ª
        time.sleep(0.5)
        
        # 3. å¯åŠ¨ ODAS è¿›ç¨‹
        if self.odas_manager.start():
            # ç­‰å¾… ODAS è¿æ¥
            time.sleep(2)
            
            if self.odas_client.is_connected():
                self._doa_enabled = True
                logger.info("âœ… DOA system started successfully")
                return True
            else:
                logger.warning("ODAS started but not connected to client")
                # ç»§ç»­è¿è¡Œï¼Œå¯èƒ½ç¨åä¼šè¿æ¥
                self._doa_enabled = True
                return True
        else:
            logger.error("Failed to start ODAS process")
            return False
    
    def stop_doa(self):
        """åœæ­¢ DOA ç³»ç»Ÿ"""
        self._doa_enabled = False
        self.odas_manager.stop()
        self.odas_client.stop()
        logger.info("DOA system stopped")
    
    def get_current_doa(self) -> Optional[float]:
        """
        è·å–å½“å‰ä¸»å£°æºçš„ DOA è§’åº¦
        
        Returns:
            æ–¹ä½è§’ (0-360Â°)ï¼Œæ— å£°æºæ—¶è¿”å› None
        """
        if not self._doa_enabled:
            return None
        
        sources = self.odas_client.get_tracked_sources(active_only=True)
        if sources:
            return sources[0].azimuth
        return None
    
    def get_tracked_sources(self) -> List[TrackedSource]:
        """è·å–æ‰€æœ‰è·Ÿè¸ªçš„å£°æº"""
        if not self._doa_enabled:
            return []
        return self.odas_client.get_tracked_sources(active_only=True)
    
    # ==================== æ³¢æŸæˆå½¢åŠŸèƒ½ ====================
    
    def record_with_beamforming(
        self, 
        max_duration: float = 30.0,
        silence_duration: float = 0.8,
        output_path: str = None
    ) -> Optional[str]:
        """
        ä½¿ç”¨æ³¢æŸæˆå½¢å½•éŸ³
        
        1. å¤šé€šé“å½•éŸ³
        2. è·å– DOA è§’åº¦
        3. æ³¢æŸæˆå½¢å¢å¼º
        4. è¾“å‡ºå•é€šé“éŸ³é¢‘
        
        Args:
            max_duration: æœ€å¤§å½•éŸ³æ—¶é•¿
            silence_duration: é™éŸ³åˆ¤å®šæ—¶é•¿
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        if not self._beamforming_enabled:
            logger.info("Beamforming disabled, using normal capture")
            return self._record_normal(max_duration, silence_duration, output_path)
        
        logger.info("Recording with beamforming...")
        
        # 1. è·å–å½“å‰ DOA è§’åº¦ä½œä¸ºåˆå§‹æ³¢æŸæŒ‡å‘
        initial_doa = self.get_current_doa()
        if initial_doa is not None:
            self.beamformer.steer(initial_doa)
            logger.info(f"Initial beam direction: {initial_doa:.1f}Â°")
        
        # 2. å¤šé€šé“å½•éŸ³
        multichannel_audio = self.multichannel_capture.record_with_vad(
            max_duration=max_duration,
            silence_duration=silence_duration
        )
        
        if multichannel_audio is None or len(multichannel_audio) == 0:
            logger.warning("No audio captured")
            return None
        
        # 3. è·å–å½•éŸ³æœŸé—´çš„å¹³å‡ DOA
        final_doa = self.get_current_doa()
        if final_doa is not None:
            beam_angle = final_doa
            logger.info(f"Final DOA: {final_doa:.1f}Â°")
        elif initial_doa is not None:
            beam_angle = initial_doa
        else:
            beam_angle = 0.0
            logger.warning("No DOA available, using 0Â°")
        
        # 4. æå–éº¦å…‹é£é€šé“ (å‰6é€šé“)
        mic_channels = multichannel_audio[:, :6]
        
        # 5. æ³¢æŸæˆå½¢
        logger.info(f"Applying beamforming at {beam_angle:.1f}Â°")
        enhanced_audio = self.beamformer.process(mic_channels, target_angle=beam_angle)
        
        # 6. ä¿å­˜ä¸º WAV æ–‡ä»¶
        if output_path is None:
            output_path = "temp_beamformed.wav"
        
        # è½¬æ¢ä¸º int16
        enhanced_int16 = (enhanced_audio * 32767).astype(np.int16)
        
        with wave.open(output_path, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)  # 16-bit
            wf.setframerate(self.config.get('audio', {}).get('sample_rate', 16000))
            wf.writeframes(enhanced_int16.tobytes())
        
        logger.info(f"Beamformed audio saved: {output_path} ({len(enhanced_audio)/16000:.2f}s)")
        return output_path
    
    def _record_normal(
        self, 
        max_duration: float = 30.0,
        silence_duration: float = 0.8,
        output_path: str = None
    ) -> Optional[str]:
        """æ™®é€šå•é€šé“å½•éŸ³ï¼ˆæ³¢æŸæˆå½¢ç¦ç”¨æ—¶çš„å›é€€ï¼‰"""
        if output_path is None:
            output_path = "temp_input.wav"
        
        audio = self.capture.record_with_vad(
            max_duration=max_duration,
            silence_duration=silence_duration,
            output_path=output_path
        )
        
        if len(audio) == 0:
            return None
        
        return output_path
    
    def enable_beamforming(self, enabled: bool = True):
        """å¯ç”¨/ç¦ç”¨æ³¢æŸæˆå½¢"""
        self._beamforming_enabled = enabled
        logger.info(f"Beamforming {'enabled' if enabled else 'disabled'}")
    
    # ==================== æœåŠ¡å™¨äº¤äº’ ====================
    
    def check_server(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å™¨è¿æ¥"""
        try:
            response = requests.get(f"{self.server_url}/health", timeout=5)
            if response.status_code == 200:
                logger.info("Server is healthy")
                return True
            else:
                logger.error(f"Server health check failed: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"Failed to connect to server: {e}")
            return False
    
    def synthesize_and_play(self, text: str, use_streaming: bool = None):
        """åˆæˆå¹¶æ’­æ”¾è¯­éŸ³"""
        if use_streaming is None:
            use_streaming = self.use_streaming_tts
        
        if use_streaming:
            return self._play_streaming_tts(text)
        else:
            return self._play_normal_tts(text)
    
    def _play_streaming_tts(self, text: str) -> bool:
        """æµå¼ TTS"""
        try:
            logger.info(f"[Streaming TTS] Requesting: {text[:30]}...")
            start_time = time.time()
            
            response = requests.post(
                f"{self.server_url}/tts/stream",
                json={"text": text},
                stream=True,
                timeout=120
            )
            
            if response.status_code != 200:
                logger.warning(f"Streaming TTS failed ({response.status_code}), falling back to normal TTS")
                return self._play_normal_tts(text)
            
            sample_rate = self.config.get('tts', {}).get('sample_rate', 22050)
            streaming_player = self.player.create_streaming_player(
                sample_rate=sample_rate,
                channels=1
            )
            
            total_bytes = 0
            first_chunk_time = None
            header_skipped = False
            
            for chunk in response.iter_content(chunk_size=4096):
                if chunk:
                    if first_chunk_time is None:
                        first_chunk_time = time.time()
                        latency = first_chunk_time - start_time
                        logger.info(f"[Streaming TTS] First audio latency: {latency:.2f}s")
                        print(f"ğŸ”Š é¦–éŸ³é¢‘å»¶è¿Ÿ: {latency:.2f}s")
                    
                    if not header_skipped and len(chunk) >= 44:
                        if chunk[:4] == b'RIFF':
                            chunk = chunk[44:]
                            header_skipped = True
                    
                    if chunk:
                        streaming_player.feed(chunk)
                        total_bytes += len(chunk)
            
            streaming_player.wait_until_done()
            
            total_time = time.time() - start_time
            logger.info(f"[Streaming TTS] Complete: {total_bytes} bytes in {total_time:.2f}s")
            return True
                
        except Exception as e:
            logger.error(f"[Streaming TTS] Error: {e}")
            return self._play_normal_tts(text)
    
    def _play_normal_tts(self, text: str) -> bool:
        """æ™®é€š TTS"""
        try:
            response = requests.post(
                f"{self.server_url}/tts",
                json={"text": text},
                timeout=120
            )
            
            if response.status_code == 200:
                temp_audio = "temp_tts_response.wav"
                with open(temp_audio, 'wb') as f:
                    f.write(response.content)
                
                self.player.play_file(temp_audio)
                Path(temp_audio).unlink(missing_ok=True)
                return True
            else:
                logger.error(f"TTS failed: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"TTS error: {e}")
            return False
    
    # ==================== å¯¹è¯åŠŸèƒ½ ====================
    
    def voice_chat_with_doa(self):
        """
        å¸¦ DOA å’Œæ³¢æŸæˆå½¢çš„è¿ç»­è¯­éŸ³å¯¹è¯æ¨¡å¼
        
        æ˜¾ç¤ºå®æ—¶å£°æºæ–¹å‘ï¼Œä½¿ç”¨æ³¢æŸæˆå½¢å¢å¼ºè¯­éŸ³
        """
        print("\n" + "=" * 60)
        print("è¯­éŸ³å¯¹è¯æ¨¡å¼ (DOA + æ³¢æŸæˆå½¢å¢å¼ºç‰ˆ)")
        print("=" * 60)
        print("è¯´è¯åä¼šè‡ªåŠ¨è¯†åˆ«å¹¶å›å¤")
        print("å®æ—¶æ˜¾ç¤ºå£°æºæ–¹å‘")
        print(f"æ³¢æŸæˆå½¢: {'âœ… å·²å¯ç”¨' if self._beamforming_enabled else 'âŒ å·²ç¦ç”¨'}")
        print("æŒ‰ Ctrl+C é€€å‡º")
        print("=" * 60)
        
        # é€‰æ‹©éŸ³è‰²
        voice_clone_id = self._select_voice_clone()
        
        try:
            while True:
                # æ˜¾ç¤ºå½“å‰ DOA çŠ¶æ€
                doa = self.get_current_doa()
                sources = self.get_tracked_sources()
                
                if sources:
                    doa_info = f"ğŸ¯ DOA: {sources[0].azimuth:.1f}Â° (activity: {sources[0].activity:.2f})"
                else:
                    doa_info = "ğŸ¯ DOA: æ— æ´»è·ƒå£°æº"
                
                print(f"\n{doa_info}")
                if self._beamforming_enabled:
                    print("ğŸ¤ è¯·å¼€å§‹è¯´è¯ (æ³¢æŸæˆå½¢å½•éŸ³)...")
                else:
                    print("ğŸ¤ è¯·å¼€å§‹è¯´è¯...")
                
                # å½•åˆ¶éŸ³é¢‘ï¼ˆæ ¹æ®è®¾ç½®ä½¿ç”¨æ³¢æŸæˆå½¢æˆ–æ™®é€šå½•éŸ³ï¼‰
                temp_audio = "temp_input.wav"
                
                if self._beamforming_enabled:
                    # ä½¿ç”¨æ³¢æŸæˆå½¢å½•éŸ³
                    audio_path = self.record_with_beamforming(
                        max_duration=30.0,
                        silence_duration=0.8,
                        output_path=temp_audio
                    )
                    if audio_path is None:
                        print("æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œç»§ç»­ç›‘å¬...")
                        continue
                else:
                    # æ™®é€šå•é€šé“å½•éŸ³
                    audio = self.capture.record_with_vad(
                        max_duration=30.0,
                        silence_duration=0.8,
                        output_path=temp_audio
                    )
                    if len(audio) == 0:
                        print("æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œç»§ç»­ç›‘å¬...")
                        continue
                
                # è·å–å½•éŸ³æ—¶çš„ DOA
                recording_doa = self.get_current_doa()
                if recording_doa is not None:
                    print(f"ğŸ“ å½•éŸ³æ—¶å£°æºæ–¹å‘: {recording_doa:.1f}Â°")
                    if self._beamforming_enabled:
                        print(f"ğŸ“¡ æ³¢æŸæŒ‡å‘: {self.beamformer._current_angle:.1f}Â°")
                
                print("å¤„ç†ä¸­...")
                
                try:
                    with open(temp_audio, 'rb') as f:
                        files = {'audio': f}
                        data = {
                            'session_id': self.session_id,
                            'voice_clone_id': voice_clone_id or '0'
                        }
                        
                        # æ·»åŠ  DOA ä¿¡æ¯åˆ°è¯·æ±‚
                        if recording_doa is not None:
                            data['doa_angle'] = str(recording_doa)
                        
                        response = requests.post(
                            f"{self.server_url}/chat",
                            files=files,
                            data=data,
                            stream=True,
                            timeout=180
                        )
                    
                    if response.status_code == 200:
                        from urllib.parse import unquote
                        asr_text = unquote(response.headers.get('X-ASR-Text', ''))
                        response_text = unquote(response.headers.get('X-Response-Text', ''))
                        emotion = response.headers.get('X-Emotion', '')
                        speaker = response.headers.get('X-Speaker', '')
                        
                        print(f"\nğŸ‘¤ ä½ : {asr_text}")
                        print(f"ğŸ˜Š æƒ…æ„Ÿ: {emotion} | ğŸ¯ è¯´è¯äºº: {speaker}")
                        print(f"ğŸ¤– åŠ©æ‰‹: {response_text}")
                        
                        # æ’­æ”¾å›å¤
                        self._play_response(response)
                        
                    else:
                        print(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                        print(response.text)
                    
                    Path(temp_audio).unlink(missing_ok=True)
                    
                except requests.exceptions.Timeout:
                    print("\nâš ï¸ è¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•")
                except Exception as e:
                    logger.error(f"Chat request failed: {e}")
                    print(f"è¯·æ±‚å¤±è´¥: {e}")
                
                time.sleep(0.5)
                
        except KeyboardInterrupt:
            print("\n\né€€å‡ºå¯¹è¯æ¨¡å¼")
    
    def _select_voice_clone(self) -> Optional[str]:
        """é€‰æ‹©éŸ³è‰²å…‹éš†"""
        try:
            response = requests.get(f"{self.server_url}/voice-clone/list", timeout=30)
            if response.status_code == 200:
                result = response.json()
                voice_clones = result.get('voice_clones', [])
                if voice_clones:
                    print("\nå¯ç”¨çš„éŸ³è‰²å…‹éš†ï¼š")
                    print("0 - ä½¿ç”¨é»˜è®¤éŸ³è‰²")
                    for idx, clone_id in enumerate(voice_clones, start=1):
                        print(f"{idx} - {clone_id}")
                    
                    choice = input("\nè¯·é€‰æ‹©éŸ³è‰² (å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
                    if choice.isdigit() and 0 < int(choice) <= len(voice_clones):
                        return voice_clones[int(choice) - 1]
            return None
        except Exception as e:
            logger.warning(f"Failed to list voice clones: {e}")
            return None
    
    def _play_response(self, response):
        """æ’­æ”¾å“åº”éŸ³é¢‘"""
        try:
            is_streaming = response.headers.get('X-Streaming-Audio', 'False') == 'True'
            
            if self.use_streaming_tts and is_streaming:
                sample_rate = self.config.get('tts', {}).get('sample_rate', 22050)
                streaming_player = self.player.create_streaming_player(
                    sample_rate=sample_rate, channels=1
                )
                
                header_skipped = False
                for chunk in response.iter_content(chunk_size=4096):
                    if not chunk:
                        continue
                    
                    if not header_skipped and len(chunk) >= 44:
                        if chunk[:4] == b'RIFF':
                            chunk = chunk[44:]
                            header_skipped = True
                    
                    if chunk:
                        streaming_player.feed(chunk)
                
                streaming_player.wait_until_done()
            else:
                response_audio = "temp_response.wav"
                with open(response_audio, 'wb') as f:
                    f.write(response.content)
                self.player.play_file(response_audio)
                Path(response_audio).unlink(missing_ok=True)
                
        except Exception as e:
            logger.error(f"Failed to play response: {e}")
    
    def doa_monitor(self):
        """
        DOA å®æ—¶ç›‘æ§æ¨¡å¼
        
        æŒç»­æ˜¾ç¤ºå£°æºæ–¹å‘
        """
        print("\n" + "=" * 60)
        print("DOA å®æ—¶ç›‘æ§æ¨¡å¼")
        print("=" * 60)
        print("æ˜¾ç¤ºå®æ—¶å£°æºå®šä½ä¿¡æ¯")
        print("æŒ‰ Ctrl+C é€€å‡º")
        print("=" * 60)
        
        try:
            frame_count = 0
            while True:
                sources = self.get_tracked_sources()
                all_sources = self.odas_client.get_tracked_sources(active_only=False)
                
                frame_count += 1
                
                if sources:
                    print(f"\nğŸ¯ æ£€æµ‹åˆ° {len(sources)} ä¸ªæ´»è·ƒå£°æº:")
                    for s in sources:
                        print(f"   å£°æº {s.id}: æ–¹ä½è§’={s.azimuth:.1f}Â°, activity={s.activity:.3f}")
                else:
                    activities = [f"{s.activity:.3f}" for s in all_sources]
                    print(f"\r[å¸§ {frame_count}] æ— æ´»è·ƒå£°æº | activity: {activities}    ", end="", flush=True)
                
                time.sleep(0.3)
                
        except KeyboardInterrupt:
            print("\n\né€€å‡ºç›‘æ§æ¨¡å¼")
    
    # ==================== ä¸»ç•Œé¢ ====================
    
    def run_interactive(self):
        """è¿è¡Œäº¤äº’å¼ç•Œé¢"""
        print("\n" + "=" * 60)
        print("è¯­éŸ³åŠ©æ‰‹å®¢æˆ·ç«¯ (DOAå¢å¼ºç‰ˆ)")
        print("=" * 60)
        
        # æ£€æŸ¥æœåŠ¡å™¨
        if not self.check_server():
            print("âš ï¸ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
        
        # å¯åŠ¨ DOA ç³»ç»Ÿ
        print("\næ­£åœ¨å¯åŠ¨ DOA ç³»ç»Ÿ...")
        if self.start_doa():
            print("âœ… DOA ç³»ç»Ÿå·²å¯åŠ¨")
        else:
            print("âš ï¸ DOA ç³»ç»Ÿå¯åŠ¨å¤±è´¥ï¼Œå°†åœ¨æ— DOAæ¨¡å¼ä¸‹è¿è¡Œ")
        
        print("\nå¯ç”¨å‘½ä»¤:")
        print("  talk   - å¸¦DOA+æ³¢æŸæˆå½¢çš„è¯­éŸ³å¯¹è¯ï¼ˆæ¨èï¼‰")
        print("  doa    - DOAå®æ—¶ç›‘æ§")
        print("  bf     - å¼€å…³æ³¢æŸæˆå½¢")
        print("  status - æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
        print("  gain   - è®¾ç½®éº¦å…‹é£å¢ç›Š")
        print("  quit   - é€€å‡º")
        print()
        
        try:
            while True:
                command = input("\nè¯·è¾“å…¥å‘½ä»¤: ").strip().lower()
                
                if command in ['quit', 'q', 'exit']:
                    print("æ­£åœ¨å…³é—­...")
                    break
                
                elif command in ['talk', 't']:
                    self.voice_chat_with_doa()
                
                elif command == 'doa':
                    self.doa_monitor()
                
                elif command == 'status':
                    self._show_status()
                
                elif command == 'gain':
                    self._configure_gain()
                
                elif command == 'bf':
                    self._toggle_beamforming()
                
                else:
                    print("æœªçŸ¥å‘½ä»¤ï¼Œè¯·é‡è¯•")
                    
        except KeyboardInterrupt:
            print("\n\nç¨‹åºè¢«ä¸­æ–­")
        finally:
            self.stop_doa()
            print("å†è§ï¼")
    
    def _show_status(self):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        print("\n" + "=" * 40)
        print("ç³»ç»ŸçŠ¶æ€")
        print("=" * 40)
        
        # æœåŠ¡å™¨çŠ¶æ€
        server_ok = self.check_server()
        print(f"æœåŠ¡å™¨: {'âœ… å·²è¿æ¥' if server_ok else 'âŒ æœªè¿æ¥'}")
        
        # ODAS çŠ¶æ€
        odas_running = self.odas_manager.is_running()
        odas_connected = self.odas_client.is_connected()
        print(f"ODAS è¿›ç¨‹: {'âœ… è¿è¡Œä¸­' if odas_running else 'âŒ æœªè¿è¡Œ'}")
        print(f"ODAS è¿æ¥: {'âœ… å·²è¿æ¥' if odas_connected else 'âŒ æœªè¿æ¥'}")
        
        # DOA çŠ¶æ€
        if self._doa_enabled:
            stats = self.odas_client.get_stats()
            print(f"DOA å¸§æ•°: {stats.get('frame_count', 0)}")
            print(f"æ´»è·ƒå£°æº: {stats.get('active_sources', 0)}")
            doa = self.get_current_doa()
            if doa is not None:
                print(f"å½“å‰ DOA: {doa:.1f}Â°")
        
        # æ³¢æŸæˆå½¢çŠ¶æ€
        print(f"æ³¢æŸæˆå½¢: {'âœ… å·²å¯ç”¨' if self._beamforming_enabled else 'âŒ å·²ç¦ç”¨'}")
        if self._beamforming_enabled:
            print(f"  æ³¢æŸæŒ‡å‘: {self.beamformer._current_angle:.1f}Â°")
        
        # å¢ç›ŠçŠ¶æ€
        gains = self.mic_gain.check_gains()
        if gains:
            print(f"éº¦å…‹é£å¢ç›Š: ADC={gains.get('adc_gain', '?')}")
        
        print("=" * 40)
    
    def _configure_gain(self):
        """é…ç½®éº¦å…‹é£å¢ç›Š"""
        print("\nå½“å‰å¢ç›Šè®¾ç½®:")
        gains = self.mic_gain.check_gains()
        print(f"  ADC PGA gain: {gains.get('adc_gain', '?')}")
        print(f"  Digital volume: {self.mic_gain.digital_volume}")
        
        try:
            adc = input("è¾“å…¥æ–°çš„ ADC å¢ç›Š (0-31, å›è½¦ä¿æŒ): ").strip()
            if adc:
                self.mic_gain.adc_gain = int(adc)
            
            digital = input("è¾“å…¥æ–°çš„æ•°å­—éŸ³é‡ (0-255, å›è½¦ä¿æŒ): ").strip()
            if digital:
                self.mic_gain.digital_volume = int(digital)
            
            self.mic_gain.set_gains()
            print("âœ… å¢ç›Šå·²æ›´æ–°")
            
        except ValueError:
            print("âŒ æ— æ•ˆçš„è¾“å…¥")
    
    def _toggle_beamforming(self):
        """å¼€å…³æ³¢æŸæˆå½¢"""
        self._beamforming_enabled = not self._beamforming_enabled
        status = "âœ… å·²å¯ç”¨" if self._beamforming_enabled else "âŒ å·²ç¦ç”¨"
        print(f"\næ³¢æŸæˆå½¢: {status}")
        
        if self._beamforming_enabled:
            print("  - ä½¿ç”¨ 6 é€šé“éº¦å…‹é£é˜µåˆ—")
            print("  - æ ¹æ® DOA è§’åº¦å¢å¼ºç›®æ ‡æ–¹å‘å£°éŸ³")
            print("  - æŠ‘åˆ¶å…¶ä»–æ–¹å‘å¹²æ‰°")
        else:
            print("  - ä½¿ç”¨å•é€šé“å½•éŸ³")
            print("  - é€‚ç”¨äºç®€å•åœºæ™¯")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Voice Assistant Client with DOA')
    parser.add_argument('--config', type=str, default='../config/config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--server', type=str, help='æœåŠ¡å™¨åœ°å€')
    parser.add_argument('--no-doa', action='store_true', help='ç¦ç”¨DOAåŠŸèƒ½')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = VoiceAssistantWithDOA(config_path=args.config)
    
    # è¦†ç›–æœåŠ¡å™¨åœ°å€
    if args.server:
        client.server_url = args.server
    
    # è¿è¡Œ
    client.run_interactive()


if __name__ == "__main__":
    main()

