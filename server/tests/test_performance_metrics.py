"""
æ€§èƒ½æŒ‡æ ‡æµ‹è¯•è„šæœ¬
æµ‹è¯• RAG æ£€ç´¢å»¶è¿Ÿã€çŸ¥è¯†å›¾è°±æŸ¥è¯¢ã€LLM æ¨ç†æ—¶é—´
"""

import time
import sys
import os

# è·å– server ç›®å½•å’Œé¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
SERVER_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROJECT_DIR = os.path.dirname(SERVER_DIR)  # voice_assistant æ ¹ç›®å½•
sys.path.insert(0, SERVER_DIR)
os.chdir(SERVER_DIR)  # åˆ‡æ¢å·¥ä½œç›®å½•åˆ° server

from typing import Dict, List
import statistics


class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.results = {
            'rag_retrieval': [],
            'knowledge_graph': [],
            'llm_inference': []
        }
    
    def measure_time(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"""
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = (time.perf_counter() - start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        return result, elapsed
    
    def add_result(self, metric_name: str, value: float):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        if metric_name in self.results:
            self.results[metric_name].append(value)
    
    def get_summary(self) -> Dict:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        summary = {}
        for name, values in self.results.items():
            if values:
                summary[name] = {
                    'count': len(values),
                    'avg': statistics.mean(values),
                    'min': min(values),
                    'max': max(values),
                    'std': statistics.stdev(values) if len(values) > 1 else 0
                }
        return summary
    
    def print_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½æŒ‡æ ‡æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        summary = self.get_summary()
        
        metrics_info = {
            'rag_retrieval': ('RAG æ£€ç´¢å»¶è¿Ÿ', '<100ms', 'FAISS å‘é‡æ£€ç´¢'),
            'knowledge_graph': ('çŸ¥è¯†å›¾è°±æŸ¥è¯¢', '<200ms', 'Neo4j Cypher æ‰§è¡Œ'),
            'llm_inference': ('LLM æ¨ç†æ—¶é—´', '~1.5s', '512 token ç”Ÿæˆ')
        }
        
        print(f"\n{'æŒ‡æ ‡':<20} | {'å¹³å‡å€¼':>10} | {'æœ€å°å€¼':>10} | {'æœ€å¤§å€¼':>10} | {'ç›®æ ‡':>10} | {'çŠ¶æ€':>6}")
        print("-" * 80)
        
        for name, info in metrics_info.items():
            if name in summary:
                s = summary[name]
                avg = s['avg']
                
                # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
                if name == 'rag_retrieval':
                    passed = avg < 100
                elif name == 'knowledge_graph':
                    passed = avg < 200
                else:  # llm_inference
                    passed = avg < 2000  # 2ç§’å†…
                
                status = "âœ… è¾¾æ ‡" if passed else "âŒ è¶…æ—¶"
                
                if name == 'llm_inference':
                    print(f"{info[0]:<18} | {avg/1000:>8.2f}s | {s['min']/1000:>8.2f}s | {s['max']/1000:>8.2f}s | {info[1]:>10} | {status}")
                else:
                    print(f"{info[0]:<18} | {avg:>8.2f}ms | {s['min']:>8.2f}ms | {s['max']:>8.2f}ms | {info[1]:>10} | {status}")
            else:
                print(f"{info[0]:<18} | {'N/A':>10} | {'N/A':>10} | {'N/A':>10} | {info[1]:>10} | âš ï¸ æœªæµ‹è¯•")
        
        print("=" * 60)


def test_rag_retrieval(metrics: PerformanceMetrics, rag_module, queries: List[str], n_runs: int = 5):
    """æµ‹è¯• RAG æ£€ç´¢å»¶è¿Ÿ"""
    print("\nğŸ” æµ‹è¯• RAG æ£€ç´¢å»¶è¿Ÿ...")
    
    for i, query in enumerate(queries):
        for run in range(n_runs):
            _, elapsed = metrics.measure_time(rag_module.retrieve, query)
            metrics.add_result('rag_retrieval', elapsed)
            print(f"  Query {i+1}, Run {run+1}: {elapsed:.2f}ms")


def test_knowledge_graph(metrics: PerformanceMetrics, kg_module, queries: List[str], n_runs: int = 5):
    """æµ‹è¯•çŸ¥è¯†å›¾è°±æŸ¥è¯¢å»¶è¿Ÿ"""
    print("\nğŸŒ æµ‹è¯•çŸ¥è¯†å›¾è°±æŸ¥è¯¢...")
    
    for i, query in enumerate(queries):
        for run in range(n_runs):
            _, elapsed = metrics.measure_time(kg_module.smart_query, query)
            metrics.add_result('knowledge_graph', elapsed)
            print(f"  Query {i+1}, Run {run+1}: {elapsed:.2f}ms")


def test_llm_inference(metrics: PerformanceMetrics, dialogue_module, queries: List[str], n_runs: int = 3):
    """æµ‹è¯• LLM æ¨ç†æ—¶é—´"""
    print("\nğŸ§  æµ‹è¯• LLM æ¨ç†æ—¶é—´...")
    
    for i, query in enumerate(queries):
        for run in range(n_runs):
            _, elapsed = metrics.measure_time(
                dialogue_module.chat, 
                query=query, 
                session_id="perf_test",
                use_rag=False
            )
            metrics.add_result('llm_inference', elapsed)
            print(f"  Query {i+1}, Run {run+1}: {elapsed/1000:.2f}s")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ NLP æ¨¡å—æ€§èƒ½æµ‹è¯•")
    print("=" * 40)
    
    metrics = PerformanceMetrics()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "æ„Ÿå†’æœ‰ä»€ä¹ˆç—‡çŠ¶",
        "é«˜è¡€å‹åƒä»€ä¹ˆè¯",
        "å¤´ç—›åº”è¯¥æŒ‚ä»€ä¹ˆç§‘"
    ]
    
    # ========================================
    # 1. æµ‹è¯• RAG æ¨¡å—
    # ========================================
    try:
        from modules.core.rag import RAGModule
        import yaml
        
        with open(os.path.join(PROJECT_DIR, 'config/config.yaml'), 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        rag_config = config.get('rag', {})
        rag = RAGModule(
            embedding_model=rag_config.get('embedding_model', 'BAAI/bge-small-zh-v1.5'),
            index_path=rag_config.get('index_path', 'data/rag_index'),
            device=rag_config.get('device', 'cpu'),
            top_k=rag_config.get('top_k', 3),
            min_score=rag_config.get('min_score', 0.5)
        )
        
        # æ£€æŸ¥ RAG ç´¢å¼•æ˜¯å¦å¯ç”¨
        if rag.index is None or (hasattr(rag.index, 'ntotal') and rag.index.ntotal == 0):
            print("âš ï¸ RAG æ¨¡å—æµ‹è¯•è·³è¿‡: FAISS ç´¢å¼•æœªæ„å»ºï¼ˆéœ€è¦å…ˆè¿è¡Œ build_rag_index.pyï¼‰")
        else:
            print(f"ğŸ“š RAG ç´¢å¼•å·²åŠ è½½: {rag.index.ntotal} æ¡æ–‡æ¡£")
            test_rag_retrieval(metrics, rag, test_queries)
        
    except Exception as e:
        print(f"âš ï¸ RAG æ¨¡å—æµ‹è¯•è·³è¿‡: {e}")
    
    # ========================================
    # 2. æµ‹è¯•çŸ¥è¯†å›¾è°±æ¨¡å—
    # ========================================
    try:
        from modules.knowledge.knowledge_graph import KnowledgeGraphModule
        
        kg = KnowledgeGraphModule(
            host="localhost",
            port=7474,
            user="neo4j",
            password="12345"
        )
        if kg.enabled:
            test_knowledge_graph(metrics, kg, test_queries)
        else:
            print("âš ï¸ çŸ¥è¯†å›¾è°±æ¨¡å—æµ‹è¯•è·³è¿‡: Neo4j æœªè¿æ¥")
        
    except Exception as e:
        print(f"âš ï¸ çŸ¥è¯†å›¾è°±æ¨¡å—æµ‹è¯•è·³è¿‡: {e}")
    
    # ========================================
    # 3. æµ‹è¯• LLM æ¨¡å— (ä½¿ç”¨ 0.5B æ¨¡å‹)
    # ========================================
    try:
        from modules.core.dialogue import DialogueModule
        import yaml
        
        with open(os.path.join(PROJECT_DIR, 'config/config.yaml'), 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        dialogue_cfg = config.get('dialogue', {})
        device = dialogue_cfg.get('device', 'cuda')  # é»˜è®¤ä½¿ç”¨ cuda
        model_name = dialogue_cfg.get('model', 'Qwen/Qwen2.5-0.5B-Instruct')
        
        print(f"\nâ³ æ­£åœ¨åŠ è½½ {model_name}ï¼Œè®¾å¤‡: {device}...")
        # DialogueModule éœ€è¦ä¼ å…¥ model å’Œ device
        dialogue = DialogueModule(
            model_name=model_name,
            device=device,
            max_length=dialogue_cfg.get('max_length', 512),
            temperature=dialogue_cfg.get('temperature', 0.7)
        )
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°: {dialogue.device}")
        test_llm_inference(metrics, dialogue, test_queries, n_runs=2)
        
    except Exception as e:
        import traceback
        print(f"âš ï¸ LLM æ¨¡å—æµ‹è¯•è·³è¿‡: {e}")
        traceback.print_exc()
    
    # ========================================
    # æ‰“å°æŠ¥å‘Š
    # ========================================
    metrics.print_report()
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    import json
    report_path = 'tests/performance_report.json'
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'summary': metrics.get_summary(),
            'raw_results': metrics.results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


if __name__ == "__main__":
    main()

